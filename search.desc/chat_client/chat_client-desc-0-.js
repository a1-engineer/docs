searchState.loadedDescShard("chat_client", 0, "Chat functionality for interacting with a large language …\nAnything that implements futures::stream::Stream with …\nSomething the model said.\nCommon chat message across all providers.\nA context prompt for the model.\nSomething that the user said.\nInfer and stream the completion of the next message in the …\nInfer the next message in the given conversation.\nGenerate a vector representation of the semantics of the …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nTry to guess the provider from the secret, and instantiate …\nAn Anthropic flavor of the completion client.\nGeneric call to any Anthropic API endpoints.\nInfer the next message in the conversation and stream the …\nInfer the next message in the conversation in one go.\nInfer the next message in the conversation.\nInfer the next message in the conversation. This is not …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nUse the default base URL and the given secret key.\nUse the given base URL and secret key.\nAn OpenAI flavor of the completion client.\nInfer the next message in the conversation. This is not …\nInfer the next message in the conversation.\nInfer the next message in the conversation.\nInfer the next message in the conversation.\nInfer the next message in the conversation. Returns a …\nGenerate a vector representation of the semantics of the …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nA lighter counterpart to the model configured in the …\nUse the default base URL and the given secret key.\nUse the given base URL and secret key.\nWhat could go wrong while completing a chat message.\nContains the error value\nContains the success value\nPartially applied std::result::Result.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.")